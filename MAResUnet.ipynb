{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66d1f42-1334-4cfb-ae19-6be6ba174a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be4d023-f462-4e7b-8352-096bd5dc1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3otherRelu(filters, kernel_size=None, stride=None, padding=None):\n",
    "    if kernel_size is None:\n",
    "        kernel_size = 3\n",
    "    if stride is None:\n",
    "        stride = 1\n",
    "    if padding is None:\n",
    "        padding = 'same'\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters, kernel_size, stride, padding, use_bias=True),\n",
    "        tf.keras.layers.ReLU()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a17fbb-d9a7-4c6f-8454-9858682c3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def l2_norm(x):\n",
    "    norm = tf.norm(x, ord=2, axis=-2, keepdims=True)\n",
    "    return tf.einsum(\"bcn, bn->bcn\", x, 1 / norm)\n",
    "\n",
    "\n",
    "\n",
    "#normalized_tensor = l2_norm(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb9c75-6c8f-4dc1-8022-c503be924f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15082c5e-1eb3-4fb1-beeb-6a2b137d6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PAM_Module(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_places, scale=8, eps=1e-6):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.gamma=tf.Variable(initial_value=tf.zeros((1,)), trainable=True)\n",
    "        self.inplaces=in_places\n",
    "        self.l2_norm=l2_norm\n",
    "        self.eps=eps\n",
    "\n",
    "        self.query_conv=tf.keras.layers.Conv2D(in_places//scale, kernel_size=1, padding='same')\n",
    "        self.key_conv=tf.keras.layers.Conv2D(in_places//scale,  kernel_size=1, padding='same')\n",
    "        self.value_conv=tf.keras.layers.Conv2D(in_places//scale,  kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, width, height, channels = x.shape\n",
    "        print(\"Shape before query_conv:\", x.shape)\n",
    "        Q= tf.reshape(self.query_conv(x), (batch_size, -1, width * height))\n",
    "        K= tf.reshape(self.key_conv(x), (batch_size, -1, width * height))\n",
    "        V= tf.reshape(self.value_conv(x), (batch_size, -1, width * height))\n",
    "        print('Q shape', Q.shape)\n",
    "        print('K shape', K.shape)\n",
    "        Q = self.l2_norm(tf.transpose(Q, perm=[0, 2, 1]))\n",
    "        K = self.l2_norm(K)\n",
    "        print('Q shape', Q.shape)\n",
    "        print('K shape', K.shape)\n",
    "        tailor_sum = 1 / (width * height + tf.einsum(\"bnc,bcm->bnm\", Q, tf.reduce_sum(K, axis=-1) + self.eps))\n",
    "        value_sum = tf.expand_dims(tf.reduce_sum(V, axis=2), axis=-1)\n",
    "        value_sum = tf.tile(value_sum, [1, 1, width * height])\n",
    "\n",
    "        matrix = tf.einsum('bmn, bcn->bmc', K, V)\n",
    "        matrix_sum = value_sum + tf.einsum(\"bnm, bmc->bcn\", Q, matrix)\n",
    "\n",
    "        weight_value = tf.einsum(\"bcn, bn->bcn\", matrix_sum, tailor_sum)\n",
    "        weight_value = tf.reshape(weight_value, (batch_size, channels, height, width))\n",
    "\n",
    "        return x + self.gamma * weight_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1f9976-a308-4968-9f4d-869db0837b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CAM_Module(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.gamma = tf.Variable(initial_value=tf.zeros((1,)), trainable=True)\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, channels = x.shape\n",
    "\n",
    "        proj_query = tf.reshape(x, (batch_size, -1, channels))\n",
    "        proj_key = tf.transpose(proj_query, perm=[0, 2, 1])\n",
    "        energy = tf.matmul(proj_query, proj_key)\n",
    "        energy_max = tf.reduce_max(energy, axis=-1, keepdims=True)\n",
    "        energy_new = energy_max - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "\n",
    "        proj_value = tf.reshape(x, (batch_size, -1, channels))\n",
    "\n",
    "        out = tf.matmul(attention, proj_value)\n",
    "        out = tf.reshape(out, (batch_size, height, width, channels))\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82791974-4c30-44e1-9450-1184b5ee5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PAM_CAM_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_ch):\n",
    "        super(PAM_CAM_Layer, self).__init__()\n",
    "        self.conv1 = conv3otherRelu(in_ch)\n",
    "\n",
    "        self.PAM = PAM_Module(in_ch)\n",
    "        self.CAM = CAM_Module()\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.conv2P = conv3otherRelu(in_ch)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "        self.conv2C = conv3otherRelu(in_ch)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.1)\n",
    "        self.conv3 = conv3otherRelu(in_ch)\n",
    "\n",
    "    def call(self, x):\n",
    "        print('shape of feature map fed to attention', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x_pam = self.PAM(x)\n",
    "        x_cam = self.CAM(x)\n",
    "        x = self.dropout1(x_pam)\n",
    "        x = self.conv2P(x)\n",
    "        x = x + self.dropout2(x_cam)\n",
    "        x = self.conv2C(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f195cb1-5ee9-4dbc-9259-261e38671396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, n_filters):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(in_channels // 4, kernel_size=1, padding='same')\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.deconv2 = tf.keras.layers.Conv2DTranspose(in_channels // 4, kernel_size=3, strides=2, padding='same', output_padding=1)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv2D(n_filters, kernel_size=1, padding='same')\n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu3 = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60023610-3a93-4f55-8f05-12dd7b64a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MAResUNet(tf.keras.Model):\n",
    "    def __init__(self, num_channels=3, num_classes=1):\n",
    "        super(MAResUNet, self).__init__()\n",
    "\n",
    "        filters = [256, 512, 1024, 2048]\n",
    "        resnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(None, None, num_channels))\n",
    "        resnet.trainable = False\n",
    "        #resnet.summary()\n",
    "        self.firstconv = resnet.get_layer('conv1_conv')\n",
    "        self.firstbn = resnet.get_layer('conv1_bn')\n",
    "        self.firstrelu = resnet.get_layer('conv1_relu')\n",
    "        self.firstmaxpool = resnet.get_layer('pool1_pool')\n",
    "        self.encoder1 = resnet.get_layer('conv2_block1_out')\n",
    "        self.encoder2 = resnet.get_layer('conv3_block4_out')\n",
    "        self.encoder3 = resnet.get_layer('conv4_block6_out')\n",
    "        self.encoder4 = resnet.get_layer('conv5_block3_out')\n",
    "\n",
    "        self.attention4 = PAM_CAM_Layer(filters[3])\n",
    "        self.attention3 = PAM_CAM_Layer(filters[2])\n",
    "        self.attention2 = PAM_CAM_Layer(filters[1])\n",
    "        self.attention1 = PAM_CAM_Layer(filters[0])\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = tf.keras.layers.Conv2DTranspose(32, 4, strides=2, padding='same')\n",
    "        self.finalrelu1 = tf.keras.layers.Activation('relu')\n",
    "        self.finalconv2 = tf.keras.layers.Conv2D(32, 3, padding='same')\n",
    "        self.finalrelu2 = tf.keras.layers.Activation('relu')\n",
    "        self.finalconv3 = tf.keras.layers.Conv2D(num_classes, 3, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.firstconv(x)\n",
    "        x1 = self.firstbn(x1)\n",
    "        x1 = self.firstrelu(x1)\n",
    "        x1 = self.firstmaxpool(x1)\n",
    "        e1 = self.encoder1(x1)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        print('output from res',e4.shape)\n",
    "        e4 = self.attention4(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + self.attention3(e3)\n",
    "        d3 = self.decoder3(d4) + self.attention2(e2)\n",
    "        d2 = self.decoder2(d3) + self.attention1(e1)\n",
    "        d1 = self.decoder1(d2)\n",
    "\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7d4054-e4fc-4ad5-865c-4b17cfc2eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from res (10, 126, 126, 64)\n",
      "shape of feature map fed to attention (10, 126, 126, 64)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"conv2d_38\" (type Conv2D).\n\nOOM when allocating tensor with shape[10,126,126,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(10, 126, 126, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m in_batch, inchannel, in_h, in_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal((in_batch, in_h, in_w, inchannel))\n\u001b[1;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pavithra\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m, in \u001b[0;36mMAResUNet.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m e4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder4(e3)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput from res\u001b[39m\u001b[38;5;124m'\u001b[39m,e4\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 47\u001b[0m e4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention4\u001b[49m\u001b[43m(\u001b[49m\u001b[43me4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[0;32m     50\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(e4) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention3(e3)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mPAM_CAM_Layer.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of feature map fed to attention\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x_pam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPAM(x)\n\u001b[0;32m     22\u001b[0m     x_cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCAM(x)\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"conv2d_38\" (type Conv2D).\n\nOOM when allocating tensor with shape[10,126,126,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(10, 126, 126, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "net = MAResUNet(3)\n",
    "in_batch, inchannel, in_h, in_w = 10, 3, 512, 512\n",
    "x = tf.random.normal((in_batch, in_h, in_w, inchannel))\n",
    "out = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edbba9-a45a-4998-b879-725fb88ca9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QueryConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_places, scale=8):\n",
    "        super(QueryConv, self).__init__()\n",
    "        self.query_conv = tf.keras.layers.Conv2D(in_places // scale, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.query_conv(x)\n",
    "\n",
    "class KeyConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_places, scale=8):\n",
    "        super(KeyConv, self).__init__()\n",
    "        self.key_conv = tf.keras.layers.Conv2D(in_places // scale, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.key_conv(x)\n",
    "\n",
    "class ValueConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_places, scale=8):\n",
    "        super(ValueConv, self).__init__()\n",
    "        self.value_conv = tf.keras.layers.Conv2D(in_places // scale, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.value_conv(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def l2_norm(x):\n",
    "    norm = tf.norm(x, ord=2, axis=-1, keepdims=True)\n",
    "    print(\"Shape before normalization:\", x.shape)\n",
    "    print(\"Shape of norm:\", norm.shape)\n",
    "    return x / norm\n",
    "\n",
    "\n",
    "# Sample input tensor\n",
    "batch_size, channels, width, height = 10, 64, 128, 128\n",
    "x = tf.random.normal((batch_size, width, height, channels))\n",
    "\n",
    "# Your provided code snippet with added print statements\n",
    "query_conv = QueryConv(in_places=channels)\n",
    "key_conv = KeyConv(in_places=channels)\n",
    "value_conv = ValueConv(in_places=channels)\n",
    "\n",
    "Q = query_conv(x)\n",
    "K = key_conv(x)\n",
    "V = value_conv(x)\n",
    "print(\"Shape after query_conv:\", Q.shape)\n",
    "print(\"Shape after key_conv:\", K.shape)\n",
    "print(\"Shape after value_conv:\", V.shape)\n",
    "\n",
    "Q = l2_norm(Q)\n",
    "K = l2_norm(K)\n",
    "print(\"Shape after l2_norm:\", Q.shape, K.shape)\n",
    "\n",
    "eps = 1e-6\n",
    "tailor_sum = 1 / (width * height + tf.einsum(\"bnc, bc->bn\", Q, tf.squeeze(tf.reduce_sum(K, axis=-1)) + eps))\n",
    "value_sum = tf.expand_dims(tf.reduce_sum(V, axis=2), axis=-1)\n",
    "value_sum = tf.tile(value_sum, [1, 1, width * height])\n",
    "print(\"Shape after einsum and expand_dims:\", tailor_sum.shape, value_sum.shape)\n",
    "\n",
    "matrix = tf.einsum('bmn, bcn->bmc', K, V)\n",
    "matrix_sum = value_sum + tf.einsum(\"bnm, bmc->bcn\", Q, matrix)\n",
    "print(\"Shape after einsum for matrix and matrix_sum:\", matrix.shape, matrix_sum.shape)\n",
    "\n",
    "weight_value = tf.einsum(\"bcn, bn->bcn\", matrix_sum, tailor_sum)\n",
    "print(\"Shape after einsum for weight_value:\", weight_value.shape)\n",
    "\n",
    "output = x + weight_value\n",
    "\n",
    "print(\"Final output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4983b-053b-4eb2-aa2a-78e54d20b054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf750e76-1fd8-45e6-86b0-2dcb6202edb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
